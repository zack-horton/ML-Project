{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3c4ee0e-cd83-495c-a388-e530ed9325ad",
   "metadata": {},
   "source": [
    "$$\n",
    "x' = \\frac{x - \\bar{x}}{\\sigma}\n",
    "$$\n",
    "\n",
    "$$\n",
    "x' = \\frac{x-min(x)}{max(x)-min(x)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09f25e0d-0347-4937-a165-4d6ddb430b3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.701956470038312e9"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb4d7441-af7e-4c8b-bb78-9c912821a87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Pkg\n",
    "# Pkg.add(\"CSV\")\n",
    "# Pkg.add(\"DataFrames\")\n",
    "# Pkg.add(\"Gurobi\")\n",
    "# Pkg.add(\"JuMP\")\n",
    "# Pkg.add(\"LinearAlgebra\")\n",
    "# Pkg.add(\"Plots\")\n",
    "# Pkg.add(\"Clustering\")\n",
    "# Pkg.add(\"StableRNGs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f89edcdb-670e-4088-a752-69edab4fe634",
   "metadata": {},
   "outputs": [],
   "source": [
    "using CSV, DataFrames, Gurobi, JuMP, LinearAlgebra, Plots, Clustering, StableRNGs, Random, Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f738a27-79bc-4346-8257-c10b655e6c8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "k_medoids_cluster (generic function with 2 methods)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function read_data(file, scaling)\n",
    "    # normalized_root = \"./normalized_data\"\n",
    "    # scaled_root = \"./scaled_data\"\n",
    "    \n",
    "    if scaling == \"normalized\"\n",
    "        data = CSV.read(\"./normalized_data/$file\", DataFrame)\n",
    "        for c in names(data, Any)\n",
    "            data[!, c]= Float64.(data[!, c])\n",
    "        end\n",
    "        return data\n",
    "    elseif scaling == \"processed\"\n",
    "        data = CSV.read(\"./processed_data/$file\", DataFrame)\n",
    "        for c in names(data, Any)\n",
    "            data[!, c]= Float64.(data[!, c])\n",
    "        end\n",
    "        return data\n",
    "    else\n",
    "        data = CSV.read(\"./scaled_data/$file\", DataFrame)\n",
    "        for c in names(data, Any)\n",
    "            data[!, c]= Float64.(data[!, c])\n",
    "        end\n",
    "        return data\n",
    "    end\n",
    "end\n",
    "\n",
    "function k_medoids_cluster(data, k, seed=0)\n",
    "    Random.seed!(seed)\n",
    "    data_matrix = Matrix(data)\n",
    "    distance_matrix = zeros(size(data_matrix)[1], size(data_matrix)[1])\n",
    "    \n",
    "    for i in 1:size(data_matrix)[1]\n",
    "        for j in 1:size(data_matrix)[1]\n",
    "            distance_matrix[i, j] = norm(data_matrix[i, :] .- data_matrix[j, :])^2\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    kmedoids_result = kmedoids(distance_matrix, k)\n",
    "    # kmedoids_result.totalcost (within-cluster sum of square distance)\n",
    "    # a = assignments(kmeans_result) # get the assignments of points to clusters\n",
    "    # c = counts(kmeans_result) # get the cluster sizes\n",
    "    # M = kmeans_result.medoids # get index number of medoids observations (ordered from k=1, ..., k=K\n",
    "    return kmedoids_result\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f74f06cd-3a8b-4333-b2ce-8ad1376a4820",
   "metadata": {},
   "outputs": [],
   "source": [
    "abalone = read_data(\"abalone_z.csv\", \"normalized\")\n",
    "similarity_prediction = read_data(\"similarity_prediction_z.csv\", \"normalized\")\n",
    "\n",
    "abalone_raw = read_data(\"abalone.csv\", \"processed\")\n",
    "similarity_prediction_raw = read_data(\"similarity_prediction.csv\", \"processed\")\n",
    "\n",
    "abalone_scaled = read_data(\"abalone_mm.csv\", \"scaled\")\n",
    "similarity_prediction_scaled = read_data(\"similarity_prediction_mm.csv\", \"scaled\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df31b1d3-93f5-4ec5-aea9-8de57cd22348",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "kmedoids_optimal (generic function with 2 methods)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function kmedoids_optimal(data, num_rows, distance, k_max, time_limit, warm_start, seed=1)\n",
    "\n",
    "    data = Matrix(data[1:num_rows, :])\n",
    "    n = size(data)[1]\n",
    "    p = size(data)[2]\n",
    "    \n",
    "    observation_range = 1:n\n",
    "    variable_range = 1:p\n",
    "    cluster_range = 1:k_max\n",
    "    big_M = 1e3\n",
    "\n",
    "    # t = time()\n",
    "    model = Model(optimizer_with_attributes(Gurobi.Optimizer, \"TimeLimit\" => time_limit, \"Seed\" => seed, \"OutputFlag\" => 0))\n",
    "\n",
    "    @variable(model,\n",
    "        m[i in observation_range, k in cluster_range], Bin)\n",
    "    @variable(model,\n",
    "        z[i in observation_range, k in cluster_range], Bin)\n",
    "    @variable(model,\n",
    "        c[k in cluster_range, j in variable_range])\n",
    "    @variable(model,\n",
    "        d[i in observation_range, j in variable_range] >= 0)\n",
    "\n",
    "    @constraint(model,\n",
    "        must_have_cluster[i in observation_range],\n",
    "        sum( (z[i, k]) for k in cluster_range) == 1)\n",
    "    @constraint(model,\n",
    "        must_have_medoid[k in cluster_range],\n",
    "        sum( (m[i, k]) for i in observation_range) == 1)\n",
    "    @constraint(model,\n",
    "        medoid_restriction_1[i in observation_range],\n",
    "        sum( (m[i, k]) for k in cluster_range) <= 1)\n",
    "    @constraint(model,\n",
    "        medoid_restriction_2[i in observation_range, k in cluster_range],\n",
    "        m[i, k] <= z[i, k])\n",
    "    \n",
    "    @constraint(model,\n",
    "        medoid_selection_UB[i in observation_range, j in variable_range, k in cluster_range],\n",
    "        c[k, j] <= data[i, j] + big_M*(1 - m[i, k]))\n",
    "    @constraint(model,\n",
    "        medoid_selection_LB[i in observation_range, j in variable_range, k in cluster_range],\n",
    "        data[i, j] - big_M*(1 - m[i, k]) <= c[k, j])\n",
    "\n",
    "    if warm_start == true\n",
    "        data_kmedoids = k_medoids_cluster(data, k_max, seed)\n",
    "        heur_medoids = data_kmedoids.medoids\n",
    "\n",
    "        m_ik = Int64.(zeros(size(data)[1], k_max))\n",
    "        z_ik = Int64.(zeros(size(data)[1], k_max))\n",
    "        c_kj = zeros(k_max, size(data)[2])\n",
    "\n",
    "        cluster = 0\n",
    "        for index in heur_medoids\n",
    "            cluster = cluster + 1\n",
    "\n",
    "            set_start_value(m[index, cluster], 1)\n",
    "            for j in variable_range\n",
    "                set_start_value(c[cluster, j], data[index, j])\n",
    "            end\n",
    "        end\n",
    "    \n",
    "        for index in 1:size(assignments(data_kmedoids))[1]\n",
    "            set_start_value(z[index, assignments(data_kmedoids)[index]], 1)\n",
    "        end\n",
    "    end\n",
    "\n",
    "    if distance == \"manhattan\"\n",
    "        @constraint(model,\n",
    "            distance_1[i in observation_range, j in variable_range, k in cluster_range],\n",
    "            d[i, j] >= data[i, j] - c[k, j] - big_M*(1 - z[i, k]))\n",
    "        @constraint(model,\n",
    "            distance_2[i in observation_range, j in variable_range, k in cluster_range],\n",
    "            d[i, j] >= c[k, j] - data[i, j] - big_M*(1 - z[i, k]))\n",
    "    else\n",
    "        @constraint(model,\n",
    "            distance_1[i in observation_range, j in variable_range, k in cluster_range],\n",
    "            d[i, j] >= (data[i, j] - c[k, j])^2 - big_M*(1 - z[i, k]))\n",
    "    end\n",
    "\n",
    "    @objective(model,\n",
    "        Min,\n",
    "        sum( sum( (d[i, j]) for j in variable_range) for i in observation_range))\n",
    "\n",
    "    optimize!(model)\n",
    "    # dt = time() - t\n",
    "    # println()\n",
    "    # println(\"Running Optimal KMedoids for following args:\")\n",
    "    # println(\"num_rows=$num_rows, distance=$distance, k_max=$k_max, time_limit=$time_limit, warm_start=$warm_start, seed=$seed\")\n",
    "    # println(\"Objective function value: $(JuMP.objective_value(model)))\")\n",
    "    # println(\"Solved in $(round(dt, digits=3)) seconds\")\n",
    "    # println(\"--------------------------------------------------------------\")\n",
    "\n",
    "    return model\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e9bd876-b11d-46b8-909f-a9a9668445ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "average_silhouette_score (generic function with 1 method)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function get_assignments(model)\n",
    "    z_values = value.(model[:z])  \n",
    "    n, k_max = size(z_values)\n",
    "    assignments = zeros(Int, n)\n",
    "\n",
    "    for i in 1:n\n",
    "        for k in 1:k_max\n",
    "            if z_values[i, k] > 0.5  \n",
    "                assignments[i] = k\n",
    "                break\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "\n",
    "    return assignments\n",
    "end\n",
    "\n",
    "function get_centers(model)\n",
    "    c_values = value.(model[:c])  \n",
    "    return c_values\n",
    "end\n",
    "\n",
    "\n",
    "\n",
    "function evaluate_clustering(data, assignments, centers, distance)\n",
    "    data_matrix = Matrix(data)\n",
    "    K = maximum(assignments)\n",
    "    n, p = size(data_matrix)\n",
    "\n",
    "    wcssd = zeros(K)\n",
    "\n",
    "    for k in 1:K\n",
    "        cluster_obs = data_matrix[assignments .== k, :]\n",
    "        for i in 1:size(cluster_obs)[1]\n",
    "            if distance == \"manhattan\"\n",
    "                wcssd[k] += sum(abs.(cluster_obs[i, :] .- centers[k, :]))^2\n",
    "            else\n",
    "                wcssd[k] += norm(cluster_obs[i, :] .- centers[k, :])^2\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "\n",
    "    return sum(wcssd)\n",
    "end\n",
    "\n",
    "\n",
    "function average_silhouette_score(data, assignments, centers)\n",
    "    data_matrix = Matrix(data)\n",
    "    n = size(data_matrix, 1)\n",
    "    K = maximum(assignments)\n",
    "\n",
    "    # Function to compute the average distance to points in a given cluster\n",
    "    function avg_distance_to_cluster(point, cluster_points)\n",
    "        return mean([norm(point - other_point) for other_point in eachrow(cluster_points)])\n",
    "    end\n",
    "\n",
    "    silhouette_scores = zeros(n)\n",
    "\n",
    "    for i in 1:n\n",
    "        # Current point and its assigned cluster\n",
    "        point = data_matrix[i, :]\n",
    "        cluster = assignments[i]\n",
    "\n",
    "        # a(i): Average distance to points in the same cluster\n",
    "        same_cluster_points = data_matrix[assignments .== cluster, :]\n",
    "        a_i = cluster == 1 ? 0 : avg_distance_to_cluster(point, same_cluster_points)\n",
    "\n",
    "        # b(i): Minimum average distance to points in other clusters\n",
    "        b_i = Inf\n",
    "        for k in 1:K\n",
    "            if k != cluster\n",
    "                other_cluster_points = data_matrix[assignments .== k, :]\n",
    "                b_i = min(b_i, avg_distance_to_cluster(point, other_cluster_points))\n",
    "            end\n",
    "        end\n",
    "\n",
    "        # Silhouette score for point i\n",
    "        silhouette_scores[i] = (b_i - a_i) / max(a_i, b_i)\n",
    "    end\n",
    "\n",
    "    return mean(silhouette_scores)\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb43135a-25a7-4d61-adb8-ff5ee494ed5b",
   "metadata": {},
   "source": [
    "# Metric Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d3d7298-e145-4dd6-bb9f-f199d0bd8356",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "run_clustering_cases (generic function with 1 method)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using DataFrames\n",
    "\n",
    "function run_clustering_cases(datasets, raw_data, proportions, k_values, time_limits, distances)\n",
    "    results = DataFrame()\n",
    "    \n",
    "     for (dataset_name, data) in datasets\n",
    "        for prop in proportions\n",
    "            num_rows = floor(Int, size(data)[1] * prop)\n",
    "            for k in k_values\n",
    "                for time_limit in time_limits\n",
    "                    for dist in distances\n",
    "                        # k_medoids_cluster(data, k, seed=0)\n",
    "                        base_result = k_medoids_cluster(data[1:num_rows, :], k)\n",
    "                        base_assignments = base_result.assignments\n",
    "                        base_centers = Matrix(data[base_result.medoids, :])  #used to be: base_centers = base_result.centers'\n",
    "                        base_wcss = evaluate_clustering(data[1:num_rows, :], base_assignments, base_centers, dist)\n",
    "                        base_silhouette = average_silhouette_score(data[1:num_rows, :], base_assignments, base_centers)\n",
    "                        \n",
    "                        base_wcss_raw = evaluate_clustering(raw_data[1:num_rows, :], base_assignments, base_centers, dist)\n",
    "                        base_silhouette_raw = average_silhouette_score(raw_data[1:num_rows, :], base_assignments, base_centers)\n",
    "                        # Add to results DataFrame\n",
    "                        push!(results, (DatasetName=dataset_name, proportion=prop, k=k, time_limit=time_limit, distance=dist, case=\"Base\", WCSS=base_wcss, WCSS_RAW=base_wcss_raw, Silhouette=base_silhouette, Silhouette_RAW=base_silhouette_raw))\n",
    "    \n",
    "                        # kmeans_optimal cases\n",
    "                        for warm_start in [false, true]\n",
    "                            try\n",
    "                                optimal_result = kmedoids_optimal(data, num_rows, dist, k, time_limit, warm_start)  #used to be: optimal_result = kmeans_optimal(data, num_rows, dist, k, time_limit, warm_start)\n",
    "                                # kmedoids_optimal(data, num_rows, distance, k_max, time_limit, warm_start, seed=1)\n",
    "                                optimal_assignments = get_assignments(optimal_result)\n",
    "                                optimal_centers = get_centers(optimal_result)\n",
    "                                optimal_wcss = evaluate_clustering(data[1:num_rows, :], optimal_assignments, optimal_centers, dist)\n",
    "                                optimal_silhouette = average_silhouette_score(data[1:num_rows, :], optimal_assignments, optimal_centers)\n",
    "    \n",
    "                                optimal_wcss_raw = evaluate_clustering(raw_data[1:num_rows, :], optimal_assignments, optimal_centers, dist)\n",
    "                                optimal_silhouette_raw = average_silhouette_score(raw_data[1:num_rows, :], optimal_assignments, optimal_centers)\n",
    "                                \n",
    "                                case_name = warm_start ? \"Optimal with Warm Start\" : \"Optimal without Warm Start\"\n",
    "                                push!(results, (DatasetName=dataset_name, proportion=prop, k=k, time_limit=time_limit, distance=dist, case=case_name, WCSS=optimal_wcss, WCSS_RAW=optimal_wcss_raw, Silhouette=optimal_silhouette, Silhouette_RAW=optimal_silhouette_raw))\n",
    "                            catch e\n",
    "                                optimal_wcss = -15095\n",
    "                                optimal_wcss_raw = -15095\n",
    "                                optimal_silhouette = -15095\n",
    "                                optimal_silhouette_raw = -15095\n",
    "                                \n",
    "                                case_name = warm_start ? \"Optimal with Warm Start\" : \"Optimal without Warm Start\"\n",
    "                                push!(results, (DatasetName=dataset_name, proportion=prop, k=k, time_limit=time_limit, distance=dist, case=case_name, WCSS=optimal_wcss, WCSS_RAW=optimal_wcss_raw, Silhouette=optimal_silhouette, Silhouette_RAW=optimal_silhouette_raw))\n",
    "                           \n",
    "                                end\n",
    "                        end\n",
    "                        \n",
    "                    end\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "\n",
    "    return results\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b972b628-63e8-4699-8dd0-9cd316c43693",
   "metadata": {},
   "outputs": [],
   "source": [
    "# abalone = read_data(\"abalone_z.csv\", \"normalized\")\n",
    "# similarity_prediction = read_data(\"similarity_prediction_z.csv\", \"normalized\")\n",
    "\n",
    "# abalone_raw = read_data(\"abalone.csv\", \"processed\")\n",
    "# similarity_prediction_raw = read_data(\"similarity_prediction.csv\", \"processed\")\n",
    "\n",
    "# abalone_scaled = read_data(\"abalone_mm.csv\", \"scaled\")\n",
    "# similarity_prediction_scaled = read_data(\"similarity_prediction_mm.csv\", \"scaled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6680b615-6874-46d2-b8af-2d54c4463c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_clustering_cases(datasets, raw_data, proportions, k_values, time_limits, distances)\n",
    "# df_similarity = run_clustering_cases([(\"normalized\", similarity_prediction), (\"scaled\", similarity_prediction_scaled)],\n",
    "#                                      similarity_prediction_raw,\n",
    "#                                   [0.25],\n",
    "#                                   [3],\n",
    "#                                   [30],\n",
    "#                                   [\"manhattan\", \"euclidean\"])\n",
    "# CSV.write(\"./model_results/kmedoids_optimal_abalone.csv\", df_similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "343a4dd0-e52e-43c8-a764-97401c49bbbc",
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: `Random` not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: `Random` not defined",
      "",
      "Stacktrace:",
      " [1] k_medoids_cluster(data::DataFrame, k::Int64, seed::Int64)",
      "   @ Main ./In[4]:27",
      " [2] k_medoids_cluster",
      "   @ ./In[4]:27 [inlined]",
      " [3] run_clustering_cases(datasets::Vector{Tuple{String, DataFrame}}, raw_data::DataFrame, proportions::Vector{Float64}, k_values::Vector{Int64}, time_limits::Vector{Int64}, distances::Vector{String})",
      "   @ Main ./In[8]:13",
      " [4] top-level scope",
      "   @ In[11]:2"
     ]
    }
   ],
   "source": [
    "# run_clustering_cases(datasets, raw_data, proportions, k_values, time_limits, distances)\n",
    "df_abalone = run_clustering_cases([(\"normalized\", abalone), (\"scaled\", abalone_scaled)],\n",
    "                                  abalone_raw,\n",
    "                                  [0.10, 0.25, 0.75],\n",
    "                                  [3, 4, 5],\n",
    "                                  [30, 90, 180],\n",
    "                                  [\"manhattan\", \"euclidean\"])\n",
    "CSV.write(\"./model_results/kmedoids_optimal_abalone.csv\", df_abalone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6de61d-66d7-4c73-be18-b8a4118fb337",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_clustering_cases(datasets, raw_data, proportions, k_values, time_limits, distances)\n",
    "df_similarity = run_clustering_cases([(\"normalized\", similarity_prediction), (\"scaled\", similarity_prediction_scaled)],\n",
    "                                     similarity_prediction_raw,\n",
    "                                     [0.10, 0.25, 0.75],\n",
    "                                     [3, 4, 5],\n",
    "                                     [30, 90, 180],\n",
    "                                     [\"manhattan\", \"euclidean\"])\n",
    "CSV.write(\"./model_results/kmedoids_optimal_similarity.csv\", df_similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68369eba-7f06-4d7f-afd9-bdf5da355537",
   "metadata": {},
   "outputs": [],
   "source": [
    "time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf60b83-4c92-41e2-b8b9-5c11c1828aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "println(\"Entire script took: $(round(time() - t, digits=3)) seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc42a0e-0e5c-433c-8976-32052dc4cfc1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.3",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
